{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['c:\\\\Users\\\\Racha\\\\Documents\\\\GitHub\\\\ChatbotPI_RAOUN\\\\BACKEND']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [21064] using StatReload\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import openai\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    " def analyze_texte(texte :str):\n",
    "    mot_cle=nltk.word_tokenize(texte)\n",
    "    return {\"sujet\":\"vide\",\"sentiments\":[],\"mot_cles\":mot_cle}\n",
    "\n",
    " def generer_reponse(texte: str):\n",
    "    return {\"reponse\":\"reponse vide\"}\n",
    "\n",
    "def formater_reponse(texte: str):\n",
    "    return {\"reponse_formater\":\"reponse vide formater\"}\n",
    "\n",
    "\n",
    "\n",
    "class AnalyseTexteInput(BaseModel):\n",
    "    texte: str\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/analyse\")\n",
    "def analyse_endpoint(analyse_input: AnalyseTexteInput):\n",
    "    print(analyse_input)\n",
    "    #miniscule\n",
    "    texte=(analyse_input.texte).lower()\n",
    "    #ponctuation\n",
    "    \n",
    "    translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text_clean= texte.translate(translation_table)\n",
    "\n",
    "    #tokenisation\n",
    "    tokens=nltk.word_tokenize(text_clean)\n",
    "    print(tokens)\n",
    "\n",
    "    #stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    print(tokens)\n",
    "    #Lemmatization\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    print(lemmatized_words)\n",
    "    query=\" \".join(lemmatized_words)+ \" In context of Computer Science\"\n",
    "    print(query)\n",
    "    response=Query_OpenIA(query)\n",
    "    return {\"msg\": response}\n",
    "\n",
    "def Query_OpenIA(query:str):\n",
    "    \n",
    "\n",
    "    openai.api_key = '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'#Please add the API key sent via email here\n",
    "    conversation = \"You are specialized in providing cooking recipes.\"+ query\n",
    "     \n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",  \n",
    "    engine=\"text-davinci-003\",  \n",
    "    prompt=conversation,\n",
    "    max_tokens=150\n",
    "    )\n",
    "\n",
    "\n",
    "    msg = response['choices'][0]['text']\n",
    "    return {\"recette\": msg}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"127.0.0.1\", port=8000, reload=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
